{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__[Twitter Dataset](https://archive.org/details/twitter_cikm_2010)__:  This dataset is a collection of scraped public twitter updates used to study the geolocation data related to twittering. We will use the first 500,000 lines of the tab separated files \"test_set_tweets.txt\" and \"training_set_tweets.txt\", where each line is of the form: UserID, TweetID, Tweet, CreatedAt.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__[Friend Network Dataset](http://socialcomputing.asu.edu/datasets/Twitter)__: This dataset is a graph dataset that represents the follower relationships among 11 million Twitter users. We will use the first 500,000 lines of the comma separated file \"edges.csv\", where each line is of the form: UserID, FollowingUserID.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use MapReduce and command line processing to compute aggregate statistics of the above data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Finding Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1)\n",
    "\n",
    "**Parse the \"test_set_tweets.txt\" to extract hashtags (i.e., anything starting with “#”), convert to lower case and remove all non-alpha numeric characters, count the occurrences of each hashtag, and return the top 10 hashtags (i.e., with largest number of occurrences).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) \n",
    "\n",
    "**Write a MapReduce approach to accomplish the above task. Include a mapper function, reducer function, and execute function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # import statements\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('twitter_cikm_2010/test_set_tweets_partial.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_hashtag(line):\n",
    "    hashtags = re.findall(r'^#(\\S+)', line.strip().lower()) # find all patterns followed by # w/ >= 1 non-whitespace chars\n",
    "    hashtags.extend(re.findall(r'\\s#(\\S+)', line.strip().lower()))\n",
    "    hashtags = [re.sub(r'\\W', '', hashtag) for hashtag in hashtags] # remove all non-alphanumeric chars in each hashtag\n",
    "    return [(hashtag, 1) for hashtag in hashtags] # return tuples containing hashtag and frequency = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_find_freq(key, vals):\n",
    "    return (key, sum(vals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapreduce_exec(data, mapper, reducer):\n",
    "    hashtag_freq_tups = map(mapper, data) # call mapper for each line of data\n",
    "    hashtag_freq_tups = np.concatenate([tup for tup in hashtag_freq_tups if len(tup) > 0]) # extend lists of tuple pairs/line and remove empty lists\n",
    "    hashtag_freq_tups = [tuple(tup) for tup in hashtag_freq_tups]\n",
    "    \n",
    "    hashtag_freq_dict = {tup[0]:[] for tup in hashtag_freq_tups} # construct dict w/ keys as hashtags and values as lists\n",
    "    for tup in hashtag_freq_tups:\n",
    "        hashtag_freq_dict[tup[0]].append(int(tup[1])) # add freq to dict value list for each hashtag\n",
    "    return sorted([reducer(tup[0], tup[1]) for tup in hashtag_freq_dict.items()], key=lambda x: x[1], reverse=True)[0:10] \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) \n",
    "\n",
    "**Report the wall clock runtime of your program when applied to the first 500,000 lines of \"test_set_tweets.txt\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.500615119934082s'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mapreduce_exec(lines, map_hashtag, reduce_find_freq)\n",
    "str(time.time() - start_time) + 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) \n",
    "\n",
    "**Report the top ten hashtags with their counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ff', 3522),\n",
       " ('nowplaying', 1799),\n",
       " ('fb', 1362),\n",
       " ('mm', 1017),\n",
       " ('fail', 628),\n",
       " ('random', 601),\n",
       " ('haiti', 586),\n",
       " ('shoutout', 516),\n",
       " ('musicmonday', 451),\n",
       " ('followfriday', 449)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapreduce_exec(lines, map_hashtag, reduce_find_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) \n",
    "\n",
    "**Write a command line program to accomplish the same task, using e.g., grep, sed, awk, sort.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```grep -oE \"\\s#\\S+|^#\\S+\" /Users/Aman/Documents/Jupyter\\ Notebooks/CS\\ 242/Project\\ 3/twitter_cikm_2010/test_set_tweets_partial.txt | tr -dc '[:alnum:]\\n' | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -r | head -10```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3522 ff\n",
    "<br>\n",
    "1799 nowplaying\n",
    "<br>\n",
    "1362 fb\n",
    "<br>\n",
    "1017 mm\n",
    "<br>\n",
    "628 fail\n",
    "<br>\n",
    "601 random\n",
    "<br>\n",
    "586 haiti\n",
    "<br>\n",
    "516 shoutout\n",
    "<br>\n",
    "451 musicmonday\n",
    "<br>\n",
    "449 followfriday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) \n",
    "\n",
    "**Run your command line program in a shell script to record the wall clock runtime.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```time grep -oE \"#\\S+\" /Users/Aman/Documents/Jupyter\\ Notebooks/CS\\ 242/Project\\ 3/twitter_cikm_2010/test_set_tweets_partial.txt | tr -dc '[:alnum:]\\n' | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -r | head -10```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "real\t0m4.421s\n",
    "<br>\n",
    "user\t0m4.547s\n",
    "<br>\n",
    "sys\t0m0.082s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) \n",
    "\n",
    "**Discuss how the runtimes compare between the two approaches.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, the UNIX runtime is faster by over a second. This could be because in the python script, two loops must be executed to search for matching patterns, while in UNIX, it can be done with one loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2)\n",
    "\n",
    "**Use Unix to join the first 500,000 lines from the file \"test_set_tweets.txt\" and the first 250,000 lines from the file \"training_set_tweets.txt\" into one file, say \"tweets.txt\". Parse this file to extract usernames (i.e.,\n",
    "anything starting with “@”), count the occurrences of each username, and return the top 10 usernames (i.e., with largest number of occurrences).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```head -250000 /Users/Aman/Documents/Jupyter\\ Notebooks/CS\\ 242/Project\\ 3/twitter_cikm_2010/training_set_tweets.txt > /Users/Aman/Documents/Jupyter\\ Notebooks/CS\\ 242/Project\\ 3/twitter_cikm_2010/training_set_tweets_partial.txt```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cat /Users/Aman/Documents/Jupyter\\ Notebooks/CS\\ 242/Project\\ 3/twitter_cikm_2010/test_set_tweets_partial.txt /Users/Aman/Documents/Jupyter\\ Notebooks/CS\\ 242/Project\\ 3/twitter_cikm_2010/training_set_tweets_partial.txt > /Users/Aman/Documents/Jupyter\\ Notebooks/CS\\ 242/Project\\ 3/twitter_cikm_2010/tweets.txt```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "**Write a MapReduce approach to accomplish the above task. Include a mapper function, reducer function, and execute function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('twitter_cikm_2010/tweets.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_username(line):\n",
    "    usernames = re.findall(r'^@(\\S+)', line.strip()) # find all patterns followed by # w/ >= 1 non-whitespace chars\n",
    "    usernames.extend(re.findall(r'\\s@(\\S+)', line.strip()))\n",
    "    return [('@' + username, 1) for username in usernames] # return tuples containing username and frequency = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_find_freq(key, vals):\n",
    "    return (key, sum(vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapreduce_exec(data, mapper, reducer):\n",
    "    username_freq_tups = map(mapper, data) # call mapper for each line of data\n",
    "    username_freq_tups = np.concatenate([tup for tup in username_freq_tups if len(tup) > 0]) # extend lists of tuple pairs/line and remove empty lists\n",
    "    username_freq_tups = [tuple(tup) for tup in username_freq_tups]\n",
    "    \n",
    "    username_freq_dict = {tup[0]:[] for tup in username_freq_tups} # construct dict w/ keys as usernames and values as lists\n",
    "    for tup in username_freq_tups:\n",
    "        username_freq_dict[tup[0]].append(int(tup[1])) # add freq to dict value list for each username\n",
    "    return sorted([reducer(tup[0], tup[1]) for tup in username_freq_dict.items()], key=lambda x: x[1], reverse=True)[0:10] \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "**Report the wall clock runtime of your program.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.473917007446289s'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mapreduce_exec(lines, map_hashtag, reduce_find_freq)\n",
    "str(time.time() - start_time) + 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "**Report the top ten usernames with their counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@RevRunWisdom:', 1229),\n",
       " ('@listensto', 939),\n",
       " ('@DonnieWahlberg', 523),\n",
       " ('@OGmuscles', 441),\n",
       " ('@addthis', 429),\n",
       " ('@breatheitin', 407),\n",
       " ('@justinbieber', 354),\n",
       " ('@MAV25', 347),\n",
       " ('@karlievoice', 304),\n",
       " ('@mtgcolorpie', 291)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapreduce_exec(lines, map_username, reduce_find_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "**Write a command line program to accomplish the same task, using e.g., grep, sed, awk, sort.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` grep -oE \"^@\\S+|\\s@\\S+\" /Users/Aman/Documents/Jupyter\\ Notebooks/CS\\ 242/Project\\ 3/twitter_cikm_2010/tweets.txt | awk '{$1=$1};1' | sort | uniq -c | sort -r | head -10```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1229 @RevRunWisdom:\n",
    "<br>\n",
    "939 @listensto\n",
    "<br>\n",
    "523 @DonnieWahlberg\n",
    "<br>\n",
    "441 @OGmuscles\n",
    "<br>\n",
    "429 @addthis\n",
    "<br>\n",
    "407 @breatheitin\n",
    "<br>\n",
    "354 @justinbieber\n",
    "<br>\n",
    "347 @MAV25\n",
    "<br>\n",
    "304 @karlievoice\n",
    "<br>\n",
    "291 @mtgcolorpie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "**Run your command line program in a shell script to record the wall clock runtime.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```time grep -oE \"@\\S+\" /Users/Aman/Documents/Jupyter\\ Notebooks/CS\\ 242/Project\\ 3/twitter_cikm_2010/tweets.txt | sort | uniq -c | sort -r | head -10```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "real\t0m18.373s\n",
    "<br>\n",
    "user\t0m18.771s\n",
    "<br>\n",
    "sys\t0m0.311s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "**Discuss how the runtimes compare between the two approaches.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UNIX runtime is approximately 10 seconds slower than the approach using MapReduce in Python. This could be the case because in UNIX, sorting is occuring twice. In the Python script, there is only one loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Within the file \"tweets.txt\", find the number of tweets that have at least two hashtags.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) \n",
    "\n",
    "**Write a MapReduce approach to accomplish the above task. Include a mapper function, reducer function, and execute function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_hashtag(line):\n",
    "    hashtags = re.findall(r'^#(\\S+)', line.strip()) # find all patterns followed by # w/ >= 1 non-whitespace chars\n",
    "    hashtags.extend(re.findall(r'\\s#(\\S+)', line.strip()))\n",
    "    #print(hashtags)\n",
    "    return [(line, 1) for hashtag in hashtags] # return tuples containing line and frequency = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_find_freq(line, hashtag_freq):\n",
    "    return (line, sum(hashtag_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapreduce_exec(data, mapper, reducer):\n",
    "    line_freq_tups = map(mapper, data) # call mapper for each line of data\n",
    "    line_freq_tups = np.concatenate([tup for tup in line_freq_tups if len(tup) > 1]) # extend lists of tuple pairs/line and remove empty lists\n",
    "    line_freq_tups = [tuple(tup) for tup in line_freq_tups]\n",
    "    \n",
    "    line_freq_dict = {tup[0]:[] for tup in line_freq_tups} # construct dict w/ keys as lines and values as lists\n",
    "    for tup in line_freq_tups:\n",
    "        line_freq_dict[tup[0]].append(int(tup[1])) # add freq to dict value list for each line\n",
    "    return len([tup[0] for tup in [reducer(tup[0], tup[1]) for tup in line_freq_dict.items()] if tup[1] >= 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "**Report the wall clock runtime of your program.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.263382911682129s'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mapreduce_exec(lines, map_hashtag, reduce_find_freq)\n",
    "str(time.time() - start_time) + 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "**Report the number of tweets with at least two hashtags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15004"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapreduce_exec(lines, map_hashtag, reduce_find_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "**Write a command line program to accomplish the same task, using e.g., grep, sed, awk, sort.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```grep -o -n -E \"^#\\S+|\\s#\\S+\" /Users/Aman/Documents/Jupyter\\ Notebooks/CS\\ 242/Project\\ 3/twitter_cikm_2010/tweets.txt | cut -d : -f 1 | uniq -c | awk '{if ($1 >= 2) {print $1}}' | wc -l```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "**Run your command line program in a shell script to record the wall clock runtime.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```time grep -o -n -E \"#\\S+\" /Users/Aman/Documents/Jupyter\\ Notebooks/CS\\ 242/Project\\ 3/twitter_cikm_2010/tweets.txt | cut -d : -f 1 | uniq -c | awk '{if ($1 >= 2) {print $1}}' | wc -l```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "real 0m2.720s\n",
    "<br>\n",
    "user 0m1.606s\n",
    "<br>\n",
    "sys 0m0.244s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "**Discuss how the runtimes compare between the two approaches.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNIX is over 5 seconds faster than MapReduce in Python. This could be because Python is using many complex internal data structures that is taking up more time, compared to UNIX which is simply reading lines and performing operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Finding Reciprocal Followers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process the follower network data to determine reciprocal following relationships, i.e., pairs of users that mutually follow each other.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "**Write a MapReduce approach to accomplish the above task. Include a mapper function, reducer function, and execute function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Twitter-dataset/data/edges_partial.csv', index_col = 0, header=None, names=['Following'])\n",
    "users = sorted(set(data.index).intersection(data['Following']))\n",
    "users_followers = {user:[] for user in users}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_users(user):\n",
    "    try:\n",
    "        if user[0] in users_followers[user.name] and user.name in users_followers[user[0]]:\n",
    "            return ((user.name, user[0]), True)\n",
    "        else:\n",
    "            return ((user.name, user[0]), False)\n",
    "    except KeyError:\n",
    "        return ((user.name, user[0]), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_users(pairs):\n",
    "    mutual_followers = [pair[0] for pair in pairs if pair[1] == True]\n",
    "    return mutual_followers\n",
    "    #return list(set(tuple(sorted(pair)) for pair in mutual_followers))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapreduce_exec(data, mapper, reducer):\n",
    "    for user in users_followers:\n",
    "        users_followers[user].extend([user for user in data.loc[user, 'Following'].values if user in users])\n",
    "            \n",
    "    pairs = data.loc[users].apply(mapper, axis=1)\n",
    "    return reducer(pairs.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "**Report the wall clock runtime of your program when applied to the first 500,000 lines of edges.csv.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.473400115966797s'"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mapreduce_exec(data, map_users, reduce_users)\n",
    "str(time.time() - start_time) + 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "**Output the results in a text file to use in the next question. This will just be a subset of the original edges.csv file. Report the difference in size between the two versions of the graph with respect to number of unique nodes, and number of edges.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with open('Twitter-dataset/data/mutuals_friends.txt', 'w') as f:\n",
    "    for pair in mapreduce_exec(data, map_users, reduce_users):\n",
    "        f.write(str(pair) + '\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File Contents:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3682, 5276)\n",
      "(5276, 3682)\n",
      "(13232, 18205)\n",
      "(13232, 63255)\n",
      "(15574, 15926)\n",
      "(15926, 15574)\n",
      "(18205, 13232)\n",
      "(19628, 19821)\n",
      "(19628, 20033)\n",
      "(19821, 19628)\n",
      "(20033, 19628)\n",
      "(22196, 76473)\n",
      "(23503, 41422)\n",
      "(31866, 32002)\n",
      "(32002, 31866)\n",
      "(32173, 32452)\n",
      "(32452, 32173)\n",
      "(33099, 62167)\n",
      "(33884, 34046)\n",
      "(33884, 34101)\n",
      "(34046, 33884)\n",
      "(34101, 33884)\n",
      "(40704, 41039)\n",
      "(40704, 40997)\n",
      "(40997, 62623)\n",
      "(40997, 40704)\n",
      "(40997, 41039)\n",
      "(40997, 201063)\n",
      "(41039, 40704)\n",
      "(41039, 40997)\n",
      "(41422, 23503)\n",
      "(58783, 58875)\n",
      "(58875, 58783)\n",
      "(60887, 70696)\n",
      "(62167, 33099)\n",
      "(62623, 40997)\n",
      "(63255, 65435)\n",
      "(63255, 13232)\n",
      "(65411, 65435)\n",
      "(65435, 65411)\n",
      "(65435, 63255)\n",
      "(65435, 93260)\n",
      "(70696, 60887)\n",
      "(70696, 70772)\n",
      "(70772, 70696)\n",
      "(76473, 22196)\n",
      "(78182, 78464)\n",
      "(78464, 78182)\n",
      "(80092, 80096)\n",
      "(80096, 80092)\n",
      "(89222, 89350)\n",
      "(89350, 89222)\n",
      "(93260, 65435)\n",
      "(93260, 93427)\n",
      "(93427, 93260)\n",
      "(100591, 100721)\n",
      "(100721, 100591)\n",
      "(102898, 122546)\n",
      "(122546, 102898)\n",
      "(134409, 134410)\n",
      "(134410, 134409)\n",
      "(135546, 135684)\n",
      "(135684, 135546)\n",
      "(192865, 192899)\n",
      "(192899, 192865)\n",
      "(201063, 40997)\n",
      "(201078, 201607)\n",
      "(201607, 201078)\n"
     ]
    }
   ],
   "source": [
    "for line in open('Twitter-dataset/data/mutuals_friends.txt').read().splitlines():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Nodes Original: 249402\n",
      "# Edges Original: 500000\n",
      "# Nodes New: 55\n",
      "# Edges New: 68\n"
     ]
    }
   ],
   "source": [
    "mutual_friend_edges = mapreduce_exec(data, map_users, reduce_users)\n",
    "print('# Nodes Original: {}'.format(len(set(data.index).union(set(data['Following'])))))\n",
    "print('# Edges Original: {}'.format(data.shape[0]))\n",
    "print('# Nodes New: {}'.format(len(set.union(*[set(pair) for pair in mutual_friend_edges]))))\n",
    "print('# Edges New: {}'.format(len(mutual_friend_edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The # of nodes and edges in the new symmetric followers graph is significantly lower than in the original graph, as presented above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "**Write a command line program to accomplish the same task, using e.g., awk, sort, join.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```head edges.csv -n 500000 | awk -F, '{if ($1 > $2){var = $1; $1 = $2; $2 = var;} print $0}' | sed 's/\\W/,/g' | sort | uniq -d```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100591,100721\n",
    "<br>\n",
    "102898,122546\n",
    "<br>\n",
    "13232,18205\n",
    "<br>\n",
    "13232,63255\n",
    "<br>\n",
    "134409,134410\n",
    "<br>\n",
    "135546,135684\n",
    "<br>\n",
    "15574,15926\n",
    "<br>\n",
    "192865,192899\n",
    "<br>\n",
    "19628,19821\n",
    "<br>\n",
    "19628,20033\n",
    "<br>\n",
    "201078,201607\n",
    "<br>\n",
    "22196,76473\n",
    "<br>\n",
    "23503,41422\n",
    "<br>\n",
    "31866,32002\n",
    "<br>\n",
    "32173,32452\n",
    "<br>\n",
    "33099,62167\n",
    "<br>\n",
    "33884,34046\n",
    "<br>\n",
    "33884,34101\n",
    "<br>\n",
    "3682,5276\n",
    "<br>\n",
    "40704,40997\n",
    "<br>\n",
    "40704,41039\n",
    "<br>\n",
    "40997,201063\n",
    "<br>\n",
    "40997,41039\n",
    "<br>\n",
    "40997,62623\n",
    "<br>\n",
    "58783,58875\n",
    "<br>\n",
    "60887,70696\n",
    "<br>\n",
    "63255,65435\n",
    "<br>\n",
    "65411,65435\n",
    "<br>\n",
    "65435,93260\n",
    "<br>\n",
    "70696,70772\n",
    "<br>\n",
    "78182,78464\n",
    "<br>\n",
    "80092,80096\n",
    "<br>\n",
    "89222,89350\n",
    "<br>\n",
    "93260,93427"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) \n",
    "\n",
    "**Run your command line program in a shell script to record the wall clock runtime.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```time head edges.csv -n 500000 | awk -F, '{if ($1 > $2){var = $1; $1 = $2; $2 = var;} print $0}' | sed 's/\\W/,/g' | sort | uniq -d```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "real 0m4.661s\n",
    "<br>\n",
    "user 0m5.778s\n",
    "<br>\n",
    "sys 0m0.239s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)\n",
    "\n",
    "**Discuss how the runtimes compare between the two approaches.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is faster that UNIX by almost 2 seconds. This is probably because in Python, we are using a subset of the data (interescting the set of people with the set of people they are following produces a set of users that contains a subset of users that are mutual friends)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Finding Friends of Friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the symmetric follower graph from the question above. For each pair of friends (i.e., pair of linked users), find the number of friends they have in common.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "**Write a map reduce approach in python to accomplish this task. Note: you will probably need two map/reduce functions to accomplish this task: one to identify the friends of each user, and another to find the friends they have in common.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Twitter-dataset/data/edges_partial.csv', index_col = 0, header=None, names=['Following'])\n",
    "mutual_friends = list(set(tuple(sorted(eval(pair))) for pair in open('Twitter-dataset/data/mutuals_friends.txt').read().splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapper:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_friends(mutual_friends):\n",
    "#     return [mutual_friends, [list(data.loc[mutual_friends[0], 'Following'].values), list(data.loc[mutual_friends[1], 'Following'].values)]]\n",
    "\n",
    "def map_friends(friends):\n",
    "    friends_of_friends = {friends[0]: [], friends[1]:[]}\n",
    "    for pair in mutual_friends:\n",
    "        if friends[0] in pair:\n",
    "            friends_of_friends[friends[0]].extend([friend for friend in pair if friend != friends[0]])\n",
    "        if friends[1] in pair:\n",
    "            friends_of_friends[friends[1]].extend([friend for friend in pair if friend != friends[1]])\n",
    "    return friends_of_friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reduce_common_friends(friends_u1, friends_u2):\n",
    "#     return len(set(friends_u1).intersection(set(friends_u2)))\n",
    "    \n",
    "def reduce_common_friends(friends_of_friends):\n",
    "    return list(set(list(friends_of_friends.values())[0]).intersection(set(list(friends_of_friends.values())[1])))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mapreduce_exec(mutual_friends):\n",
    "#     friends = list(map(map_friends, mutual_friends))\n",
    "#     friends_of_friends = {mut_friend:0 for mut_friend in mutual_friends}\n",
    "#     for mut_friends in friends:\n",
    "#         friends_of_friends[mut_friends[0]] = reduce_common_friends(mut_friends[1][0], mut_friends[1][1])\n",
    "#     return sorted(friends_of_friends.items(), key=lambda kv: kv[1], reverse=True)[0:10]\n",
    "\n",
    "def mapreduce_exec(mutual_friends):\n",
    "    friends_of_friends = list(map(map_friends, mutual_friends))\n",
    "    common_friends = {tuple(pair.keys()):0 for pair in friends_of_friends}\n",
    "    for friends in friends_of_friends:\n",
    "        common_friends[tuple(friends.keys())] = len(reduce_common_friends(friends))        \n",
    "    return sorted(common_friends.items(), key=lambda kv: kv[1], reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "**Report the top ten pairs and how many friends they have in common.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Friends: (40997, 41039), # Common Friends: 1\n",
      "Mutual Friends: (40704, 40997), # Common Friends: 1\n",
      "Mutual Friends: (40704, 41039), # Common Friends: 1\n",
      "Mutual Friends: (22196, 76473), # Common Friends: 0\n",
      "Mutual Friends: (33884, 34046), # Common Friends: 0\n",
      "Mutual Friends: (89222, 89350), # Common Friends: 0\n",
      "Mutual Friends: (93260, 93427), # Common Friends: 0\n",
      "Mutual Friends: (33884, 34101), # Common Friends: 0\n",
      "Mutual Friends: (33099, 62167), # Common Friends: 0\n",
      "Mutual Friends: (58783, 58875), # Common Friends: 0\n"
     ]
    }
   ],
   "source": [
    "for entry in mapreduce_exec(mutual_friends):\n",
    "    print('Mutual Friends: {}, # Common Friends: {}'.format(entry[0], entry[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
